<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/baby.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/baby.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/baby.png">
  <link rel="mask-icon" href="/images/baby.png" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Palatino:300,300italic,400,400italic,700,700italic|Monaco:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"thesong96.github.io","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"default"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="https:&#x2F;&#x2F;netron.app&#x2F; Hi Nansong, I reviewed your application for our Robotics Perception Engineer job and am impressed with your background. Our work in DL will involve object detection, pose estimatio">
<meta property="og:type" content="article">
<meta property="og:title" content="HarDNet">
<meta property="og:url" content="https://thesong96.github.io/2021/08/07/HarDNet/index.html">
<meta property="og:site_name" content="TheSong">
<meta property="og:description" content="https:&#x2F;&#x2F;netron.app&#x2F; Hi Nansong, I reviewed your application for our Robotics Perception Engineer job and am impressed with your background. Our work in DL will involve object detection, pose estimatio">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.imgur.com/4ctG2vA.jpg">
<meta property="og:image" content="https://i.imgur.com/6Dd3YCK.gif">
<meta property="og:image" content="https://i.imgur.com/W9uShbx.gif">
<meta property="og:image" content="https://i.imgur.com/J9PMp5H.png">
<meta property="og:image" content="https://i.imgur.com/PMMlejN.png">
<meta property="og:image" content="https://i.imgur.com/tZK5ul3.png">
<meta property="og:image" content="https://i.imgur.com/p9Ma0pN.png">
<meta property="og:image" content="https://i.imgur.com/XdQOr5z.png">
<meta property="og:image" content="https://i.imgur.com/wuwFmdC.png">
<meta property="og:image" content="https://i.imgur.com/ILpUprn.png">
<meta property="og:image" content="https://i.imgur.com/m4xfneZ.png">
<meta property="og:image" content="https://i.imgur.com/2h6AWHT.png">
<meta property="og:image" content="https://i.imgur.com/LGEETks.png">
<meta property="og:image" content="https://i.imgur.com/2WshhJ1.png">
<meta property="og:image" content="https://i.imgur.com/BAiGv6u.png">
<meta property="og:image" content="https://i.imgur.com/q0FFtdA.png">
<meta property="og:image" content="https://i.imgur.com/gpsIfFp.png">
<meta property="og:image" content="https://i.imgur.com/9N1L59Y.png">
<meta property="og:image" content="https://i.imgur.com/tBX5IOD.png">
<meta property="og:image" content="https://i.imgur.com/qNrloFH.png">
<meta property="og:image" content="https://i.imgur.com/OvhT14h.png">
<meta property="og:image" content="https://i.imgur.com/7Bp52ti.png">
<meta property="og:image" content="https://i.imgur.com/NEHLFWJ.png">
<meta property="og:image" content="https://i.imgur.com/f2qVmVS.png">
<meta property="og:image" content="https://i.imgur.com/iK3gljS.png">
<meta property="og:image" content="https://i.imgur.com/UP8a1OS.png">
<meta property="og:image" content="https://i.imgur.com/WGbHRho.png">
<meta property="article:published_time" content="2021-08-07T22:16:56.000Z">
<meta property="article:modified_time" content="2021-10-14T08:15:52.547Z">
<meta property="article:author" content="Nansong Yi">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.imgur.com/4ctG2vA.jpg">

<link rel="canonical" href="https://thesong96.github.io/2021/08/07/HarDNet/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>HarDNet | TheSong</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/rss.xml" title="TheSong" type="application/rss+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">TheSong</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

      
    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://thesong96.github.io/2021/08/07/HarDNet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/baby.png">
      <meta itemprop="name" content="Nansong Yi">
      <meta itemprop="description" content="TheSong is named after IG.TheShy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TheSong">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          HarDNet
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-08-07 15:16:56" itemprop="dateCreated datePublished" datetime="2021-08-07T15:16:56-07:00">2021-08-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-10-14 01:15:52" itemprop="dateModified" datetime="2021-10-14T01:15:52-07:00">2021-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Object-Detection/" itemprop="url" rel="index"><span itemprop="name">Object Detection</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>https://netron.app/</p>
<p>Hi Nansong,</p>
<p>I reviewed your application for our Robotics Perception Engineer job and am impressed with your background. Our work in DL will involve object detection, pose estimation, distance measurement and planning/control using monocular cameras. We use CenterNet-HarDNet to train the network for our detection job. You can find the original repo here:https://github.com/PingoLH/CenterNet-HarDNet In the above repo, the author has provided a tutorial for training and trained 85-layer network on COCO dataset with 80 classes. Your tasks are:</p>
<ol type="1">
<li>Please read the papers (links in the repo) and briefly discuss with us what’s special about this network and how it is different from YOLO, SSD etc. What’s its advantages and disadvantages in your opinion?</li>
<li>Suppose the platform we need to deploy on is an edge device, so we need to train a 68-layer network with only 3 categories: Person, Animal, Vehicles. For animals we want to include dogs and cats. For vehicles we want to include cars and trucks. Please modify the code in the repo and train a network with above specs. The network doesn’t need to be fully trained. Just show your modified training pipeline works. Note: The author has included a 68-layer architecture in the code.</li>
<li>Bonus: Can you implement mix precision training in the code? You can share your update by Wednesday. We can then have a Zoom interview to take your application to the next stage.</li>
</ol>
<p><a href="https://github.com/PingoLH/CenterNet-HarDNet" target="_blank" rel="noopener"><img src="https://i.imgur.com/4ctG2vA.jpg" alt="img" /></a></p>
<p><a href="https://github.com/PingoLH/CenterNet-HarDNet" target="_blank" rel="noopener">PingoLH/CenterNet-HarDNetgithub.com</a></p>
<h2 id="key-words">Key words</h2>
<ul>
<li>https://yskim0.github.io/paper%20review/2020/09/11/HarDNet/</li>
<li>https://blog.csdn.net/qq_19784349/article/details/114545572</li>
<li>https://blog.csdn.net/honyniu/article/details/86695574?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-4.base&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-4.base</li>
<li>https://blog.csdn.net/jacke121/article/details/104649810?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-10.base&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-10.base</li>
<li>https://www.cnblogs.com/king-lps/p/9559335.html</li>
</ul>
<h1 id="centernet-hardnet">CenterNet-HarDNet</h1>
<h2 id="centernet-objects-as-points">CenterNet: Objects as Points:</h2>
<p><a href="https://arxiv.org/pdf/1904.07850.pdf" target="_blank" rel="noopener">paper link</a></p>
<p>https://medium.com/visionwizard/centernet-objects-as-points-a-comprehensive-guide-2ed9993c48bc</p>
<ul>
<li>Our center point based approach, CenterNet, is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors.</li>
<li>Achieves the best trade-off between speed and accuracy.</li>
<li>First, our CenterNet assigns the “anchor” based solely on location, not box overlap [18]. We have no manual thresholds [18] for foreground and background classification.</li>
<li>Second, we only have one positive “anchor” per object, and hence do not need NonMaximum Suppression (NMS) [2]. We simply extract local peaks in the keypoint heatmap [4, 39].</li>
<li>Third, CenterNet uses a larger output resolution (output stride of 4) compared to traditional object detectors [21, 22] (output stride of 16). This eliminates the need for multiple anchors [47].</li>
</ul>
<h2 id="hardnet">HarDNet</h2>
<p>https://arxiv.org/abs/1909.00948</p>
<h3 id="densenet">DenseNet</h3>
<ul>
<li><p><a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="noopener">paper link</a></p></li>
<li><p>https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803</p></li>
</ul>
<h4 id="overview">Overview</h4>
<ul>
<li>Comparing with <strong>Standard ConvNet</strong> and <strong>ResNet</strong>, in <strong>DenseNet</strong>, each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers. <strong>Concatenation</strong> is used. <strong>Each layer is receiving a “collective knowledge” from all preceding layers</strong>.</li>
<li>higher computational efficiency and memory efficiency.</li>
</ul>
<figure>
<img src="https://i.imgur.com/6Dd3YCK.gif" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<h4 id="densenet-architecture">DenseNet Architecture</h4>
<ul>
<li>Basic DenseNet Composition Layer</li>
</ul>
<figure>
<img src="https://i.imgur.com/W9uShbx.gif" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>For each composition layer, <strong>Pre-Activation</strong> <a href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" target="_blank" rel="noopener"><strong>Batch Norm (BN)</strong></a> <strong>and ReLU, then 3×3 Conv</strong> are done with output feature maps of <em>k</em> channels, say for example, to transform <em>x</em>0, <em>x</em>1, <em>x</em>2, <em>x</em>3 to <em>x</em>4. This is the idea from <a href="https://towardsdatascience.com/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e" target="_blank" rel="noopener"><em>Pre-Activation ResNet</em></a><em>.</em></p>
<h4 id="densenet-b-bottleneck-layers">DenseNet-B (Bottleneck Layers)</h4>
<figure>
<img src="https://i.imgur.com/J9PMp5H.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<figure>
<img src="https://i.imgur.com/PMMlejN.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>To reduce the model complexity and size, <strong>BN-ReLU-1×1 Conv</strong> is done before <strong>BN-ReLU-3×3 Conv</strong>.</p>
<h4 id="result">Result</h4>
<ul>
<li><p>compare with <a href="https://towardsdatascience.com/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e" target="_blank" rel="noopener"><em>Pre-Activation ResNet</em></a> <img src="https://i.imgur.com/tZK5ul3.png" alt="img" /></p></li>
<li><p>But <strong>why it can be better by keeping the shortcut connection path clean</strong> (by moving the ReLU layer from shortcut connection path to conv layer path as in the figure)?</p></li>
</ul>
<h3 id="downsides-of-previous-detection-algorithms">Downsides of previous detection algorithms</h3>
<ul>
<li>Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, inefficient, and requires additional post-processing.
<ul>
<li><strong>one stage</strong>: slide a complex arrangement of possible bounding boxes, called anchors, over the image and classify them directly without specifying the box content. Sliding window based object detectors are however a bit wasteful, as they need to enumerate all possible object locations and dimensions.</li>
<li><strong>two stage</strong>: recompute image features for each potential box proposed by xxx?????????? algorithm, then classify those features.</li>
<li><strong>Post-processing</strong>: namely non-maxima suppression, then removes duplicated detections for the same instance by computing bounding box IoU. This post-processing is hard to differentiate and train, hence most current detectors are not end-to-end trainable.</li>
</ul></li>
<li></li>
</ul>
<h2 id="focal-loss">Focal Loss</h2>
<ul>
<li><p>主要是用于解决data imbalance: foreground and background class imbalance.</p></li>
<li><p><a href="https://arxiv.org/pdf/1708.02002.pdf" target="_blank" rel="noopener">paper</a></p></li>
<li><p>https://medium.com/visionwizard/understanding-focal-loss-a-quick-read-b914422913e7</p></li>
</ul>
<h1 id="object-detection-metrics">Object Detection Metrics</h1>
<p><a href="https://github.com/rafaelpadilla/Object-Detection-Metrics" target="_blank" rel="noopener">Academic Paper</a></p>
<h2 id="intersection-over-union">Intersection over Union</h2>
<figure>
<img src="https://i.imgur.com/p9Ma0pN.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>IoU metric ranges from 0 and 1 with 0 signifying no overlap and 1 implying perfect overlap between <code>*gt*</code> and <code>*pd*</code>. With IoU metric we need to define a threshold (α, say) that is used to distinguish a valid detection from the one which is not.</p>
<p>We can, therefore, redefine <strong>TP</strong> (correct detection) as a detection for which IoU≥ α and <strong>FP</strong> (invalid detection) with IoU&lt; α. <strong>FN</strong> is a ground-truth missed by the model.</p>
<p>For example, at IoU threshold, α = 0.5 (or 50%), we can define TP, FP and FN as shown in the Fig 4 below.</p>
<figure>
<img src="https://i.imgur.com/XdQOr5z.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>Note:</strong> If we raise IoU threshold above 0.86, the first instance will be a FP and if we lower IoU threshold below 0.24, the second instance becomes TP.</p>
<h2 id="precision-and-recall">Precision and Recall</h2>
<p>As started earlier TN are not used in object detection problems and therefore one as to avoid metrics based on this confusion matrix component such as True Negative Rate (TNR), False Positive Rate (FPR), Negative Predictive Value (NPC) and Receiver Operating Characteristic (ROC) curve. Instead, the evaluation of object detection models based on Precision (P) and Recall (R) which are defined as</p>
<figure>
<img src="https://i.imgur.com/wuwFmdC.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>Equation 1: Precision and Recall</p>
<p><strong>Precision</strong> is the ability of a classifier to identify relevant objects only. It is the proportion of true positive detections.</p>
<p><strong>Recall</strong>, on the other hand, measures the ability of the model to find all relevant cases (that is, all ground-truths) — the proportion of true positives detected among all ground-truths.</p>
<p>A good model is a model that can identify most ground-truth objects (high recall) while only finding the relevant objects (high precision) often. A perfect model is the one with FN=0 (recall=1) and FP=0 (precision=1). The former is usually the objective, the latter often unattainable.</p>
<h2 id="precision-x-recall-curve-pr-curve">Precision x Recall Curve (PR Curve)</h2>
<p>The precision-recall (PR) curve is a plot of precision as a function of recall. It shows trade-off between the two metrics for varying confidence values for the model detections. If the FP is low, the precision is high but more object instances may be missed yielding high FN — low recall. Conversely, if one accepts more positives by lowering IoU threshold, α, the recall will increase but false positives may also increase, decreasing the precision score. For a good model, both precision and recall should remain high even if the confidence threshold varies.</p>
<h2 id="average-precision">Average Precision</h2>
<p>AP@α is, ideally, the Area Under the PR Curve (AUC-PR). Mathematically, AP is defined as</p>
<figure>
<img src="https://i.imgur.com/ILpUprn.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>Equation 2: Definition of Average Precision</p>
<p><strong>Notation:</strong> AP@α or APα means Average Precision(AP) at IoU threshold of α. Therefore AP50 and A75 means AP at IoU threshold of 50% and 75% respectively.</p>
<p>A high AUC-PR implies high precision and high recall. Naturally, often, PR curve is a zigzag-like plot (not monotonically decreasing). We often want to remove this behavior (make the curve to be monotonically decreasing) before calculating the Area Under the Curve (the Average Precision). This is done using interpolation methods. We will discuss two of those interpolation methods below:</p>
<ol type="1">
<li>11-point interpolation method</li>
<li>All-point interpolation approach</li>
</ol>
<h3 id="point-interpolation-method"><strong>11-point interpolation method</strong></h3>
<p>A 11-point AP is a plot of interpolated precision scores for a model results at 11 equally spaced standard recall levels, namely, 0.0, 0.1, 0.2, . . . 1.0. It is defined as</p>
<figure>
<img src="https://i.imgur.com/m4xfneZ.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>Equation 3: 11-point interpolation formula</p>
<p>where, R = {0.0, 0.1, 0.2, . . . 1.0} and</p>
<figure>
<img src="https://i.imgur.com/2h6AWHT.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>Equation 4 : Supporting equation 3</p>
<p>that is, <strong>interpolated precision at recall value, r —</strong> It is the highest precision for any recall value r’≥ r. If this one doesn’t make sense, I promise you that it will make full sense once we go through an example.</p>
<h3 id="all-point-interpolation-method"><strong>All — point interpolation method</strong></h3>
<p>Unlike 11-point, all-point interpolation interpolates through all the positions, that is</p>
<figure>
<img src="https://i.imgur.com/LGEETks.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<h2 id="mean-average-precision-map">Mean Average Precision (mAP)</h2>
<p><strong>Remark (AP and the number of classes):</strong> AP is calculated individually for each class. This means that there are as many AP values as the number of classes (loosely). These AP values are averaged to obtain the metric: <strong>mean Average Precision (mAP)</strong>. Precisely, mean Average Precision (mAP) is the average of AP values over all classes.</p>
<figure>
<img src="https://i.imgur.com/2WshhJ1.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<figure>
<img src="https://i.imgur.com/BAiGv6u.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>Remark (AP and IoU)</strong>: As said earlier, AP is calculated at a given IoU threshold, α. With this reasoning, AP can be calculated over a range of thresholds. <a href="https://arxiv.org/pdf/1405.0312.pdf" target="_blank" rel="noopener">Microsoft COCO</a>, calculated AP of a given category/class at 10 different IoU ranging from 50% to 95% at 5% step-size, usually denoted, AP@[.50:.5:.95]. <a href="https://arxiv.org/pdf/1703.06870.pdf" target="_blank" rel="noopener">Mask R-CNN</a>, reports the average of AP@[.50:.5:.95] simply as AP. It says</p>
<blockquote>
<p>“We report the standard COCO metrics including AP (averaged over IoU thresholds), AP50,AP75, and APS, APM, APL(AP at different scales)” — Extract from Mask R-CNN paper</p>
</blockquote>
<p>To make all these things clearer, let us go through an example.</p>
<h2 id="map计算实例">mAP计算实例</h2>
<p>下面这个blog把object detection里面的AP和mAP讲清楚了</p>
<ul>
<li>https://towardsdatascience.com/on-object-detection-metrics-with-worked-example-216f173ed31e</li>
</ul>
<p>Consider Fig 5 below. It containing 3 images with 12 detections (red bounding boxes) and 9 ground truths(green boxes). Each detection is labelled with a letter and confidence of the prediction. In this example we are considering that all the ground-truths are of the same class (detecting object of the same class) and we will use IoU threshold, α = 50%. The IoU for each detection-truth pair is indicated in Table 1 below. The columns cumTP and cumFP are cumulative values for TP and FP columns respectively. It accumulate TP and FP values above the corresponding confidence level.</p>
<figure>
<img src="https://i.imgur.com/q0FFtdA.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>Fig 5 : A model detecting objects of the same class. There are 12 detections and 9 ground-truths.</p>
<figure>
<img src="https://i.imgur.com/gpsIfFp.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>Remark (Multiple detections)</strong>: In some cases, there are multiple detections overlapping one ground- truth, e.g. <strong>c,d in image 1</strong>, <strong>g,f in image 2</strong> and <strong>i,k in image 3</strong>. For such cases, a detection with the highest confidence is considered as TP and the rest of the detections as FPs. This is, however, conditioned on the IoU threshold. The detection with the highest confidence should have IoU&gt; threshold otherwise all detections are FPs. After applying this argument</p>
<ul>
<li><strong>c and d becomes FPs</strong> because none of them meets the threshold requirement. <em>c</em> and <em>d</em> have 47% and 42% IoUs respectively against the required 50%.</li>
<li><strong>g is a TP and f is FP</strong>. Both have IoUs greater than 50% but <em>g</em> has a higher confidence of 97% against the confidence of 96% for <em>f</em>.</li>
<li>What about <strong>i</strong> and <strong>k</strong>?</li>
</ul>
<blockquote>
<p>Multiple detections of the same object in an image were considered false detections e.g. 5 detections of a single object counted as 1 correct detection and 4 false detections — <strong>Source: PASCAL VOC 2012 paper</strong>.</p>
<p>Some detectors can output multiple detections overlapping a single ground truth. For those cases the detection with the highest confidence is considered a TP and the others are considered as FP, as applied by the PASCAL VOC 2012 challenge. -Source: <strong>A Survey on Performance Metrics for Object-Detection Algorithms paper.</strong></p>
</blockquote>
<figure>
<img src="https://i.imgur.com/9N1L59Y.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>Table 1 : Computation of precision and recall with IoU threshold, α = 50%. IoU is used to determine whether a detection is TP or FP. cumTP and cumFP are cumulative values for TP and FP columns respectively. The all_detections columns is obtained by summing cumTP and cumFP values.</p>
<p><strong>Important:</strong> Before filling up <em>cumTP, cumFP, all_detection, precision and recall</em> you need to sort the table values by confidence, in descending order. precision is cumTP/all_detections and recall is cumTP/number_of_ground_truths. We have 9 ground-truths.</p>
<h3 id="point-interpolation"><strong>11-point interpolation</strong></h3>
<p>To calculate the approximation of AP@0.5 using 11-point interpolation, we need to average the precision values for recall values in R (see Equation 3) , that is, for recall values 0.0, 0.1, 0.2, . . . 1.0. as shown in Fig 6 Right below.</p>
<figure>
<img src="https://i.imgur.com/tBX5IOD.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<figure>
<img src="https://i.imgur.com/qNrloFH.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<figure>
<img src="https://i.imgur.com/OvhT14h.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>Fig 6: Left: Plot of all precision-recall values. Right: 11 precision values matching the following recall values: 0.0, 0.1, …, 1.0.</p>
<h3 id="all-point-interpolation">All-point interpolation</h3>
<p>From the definition in Equation 5, we can calculate AP@50 using all-point interpolation as follows</p>
<figure>
<img src="https://i.imgur.com/7Bp52ti.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<figure>
<img src="https://i.imgur.com/NEHLFWJ.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<figure>
<img src="https://i.imgur.com/f2qVmVS.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>Fig 7: Left: All point interpolation curve(in red) overlaying the original PR curve. Right: Regions for all-point interpolation</p>
<p>Put simply, all-point interpolation involves calculating and summing area values of the 4 regions (R1, R2, R3 and R4) in Figure 1.3b,that is,</p>
<figure>
<img src="https://i.imgur.com/iK3gljS.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>And that is all!</p>
<p><strong>Remark:</strong> Recall that, we said that AP is calculated for each class. Our calculations is for one class. For several classes we can calculate mAP by simply taking the average of the resulting AP values for the different class.</p>
<h2 id="the-coco-challenges-variants">The COCO challenge’s variants</h2>
<p>https://blog.zenggyu.com/en/post/2018-12-16/an-introduction-to-evaluation-metrics-for-object-detection/</p>
<p>The COCO challenge's variants Recall that the Pascal VOC challenge defines the mAP metric using a single loU threshold of <span class="math inline">\(0.5\)</span>. However, the COCO challenge defines several mAP metrics using different thresholds, including:</p>
<ul>
<li><span class="math inline">\(m A P^{I o U=.50: .05: .95}\)</span> which is mAP averaged over 10 loU thresholds (i.e., <span class="math inline">\(0.50,0.55,0.60, \ldots, 0.95)\)</span> and is the primary challenge metric;</li>
<li><span class="math inline">\(m A P^{I o U=.50}\)</span>, which is identical to the Pascal VOC metric;</li>
<li><span class="math inline">\(m A P^{I o U=.75}\)</span>, which is a strict metric.</li>
</ul>
<p>In addition to different loU thresholds, there are also mAP calculated across different object scales; these variants of <span class="math inline">\(\mathrm{mAP}\)</span> are all averaged over 10 loU thresholds (i.e., <span class="math inline">\(0.50,0.55,0.60, \ldots, 0.95\)</span> ):</p>
<ul>
<li><span class="math inline">\(m A P^{\text {small }}\)</span>, which is mAP for small objects that covers area less than <span class="math inline">\(32^{2}\)</span>;</li>
<li><span class="math inline">\(m A P^{\text {medium }}\)</span>, which is <span class="math inline">\(\mathrm{mAP}\)</span> for medium objects that covers area greater than <span class="math inline">\(32^{2}\)</span> but less than <span class="math inline">\(96^{2}\)</span>;</li>
<li><span class="math inline">\(m A P^{\text {large }}\)</span>, which is <span class="math inline">\(\mathrm{mAP}\)</span> for large objects that covers area greater than <span class="math inline">\(96^{2}\)</span></li>
</ul>
<p>Like mAP, the mAR metric also has many variations. One set of mAR variants vary across different numbers of detections per image:</p>
<ul>
<li><span class="math inline">\(m A R^{m a x}=1\)</span>, which is mAR given 1 detection per image;</li>
<li><span class="math inline">\(m A R^{\max =10}\)</span>, which is mAR given 10 detections per image;</li>
<li><span class="math inline">\(m A R^{\max =100}\)</span>, which is mAR given 100 detections per image.</li>
</ul>
<p>The other set of mAR variants vary across the size of detected objects:</p>
<ul>
<li><span class="math inline">\(m A R^{\text {small }}\)</span>, which is mAR for small objects that covers area less than <span class="math inline">\(32^{2}\)</span>;</li>
<li><span class="math inline">\(m A R^{\text {medium }}\)</span>, which is mAR for medium objects that covers area greater than <span class="math inline">\(32^{2}\)</span> but less than <span class="math inline">\(96^{2}\)</span>;</li>
<li><span class="math inline">\(m A R^{\text {large }}\)</span>, which is <span class="math inline">\(\mathrm{mAR}\)</span> for large objects that covers area greater than <span class="math inline">\(96^{2}\)</span>.</li>
</ul>
<h2 id="understanding-coco-dataset">Understanding COCO dataset</h2>
<p>https://majianglin2003.medium.com/object-detection-datasets-coco-ebadb64a21e4</p>
<figure>
<img src="https://i.imgur.com/UP8a1OS.png" alt="image-20210825181629702" /><figcaption aria-hidden="true">image-20210825181629702</figcaption>
</figure>
<figure>
<img src="https://i.imgur.com/WGbHRho.png" alt="image-20210829050828992" /><figcaption aria-hidden="true">image-20210829050828992</figcaption>
</figure>
<p>Big Goals:</p>
<ul>
<li>for daily affordable and sanitary food</li>
<li>pursuit delicious/high quility food</li>
<li>full-stack robots, from preparing food material to cooking, serving and clean all the dishes.</li>
</ul>
<ol type="1">
<li>Problem: now the cooking robots can only make pre-defined food. Tik Tok, connect it with our robots.</li>
<li>recipe repository, like github</li>
<li>How to create a complete document/protocol to define the material, procedure of cooking.</li>
</ol>

    </div>

    
    
    
        <div class="reward-container">
  <div>谢谢老板🐱~.</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    Donate
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/reward.png" alt="Nansong Yi WeChat Pay">
        <p>WeChat Pay</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.png" alt="Nansong Yi Alipay">
        <p>Alipay</p>
      </div>

  </div>
</div>

        

  <div class="followme">
    <p>Welcome to my other publishing channels</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/rss.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/11/03/DAYLY-PlAN/" rel="prev" title="DAYLY Summary and Plan">
      <i class="fa fa-chevron-left"></i> DAYLY Summary and Plan
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/09/25/Deep-Reinforcement-Learning/" rel="next" title="Deep Reinforcement Learning">
      Deep Reinforcement Learning <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#key-words"><span class="nav-number">1.</span> <span class="nav-text">Key words</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#centernet-hardnet"><span class="nav-number"></span> <span class="nav-text">CenterNet-HarDNet</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#centernet-objects-as-points"><span class="nav-number">1.</span> <span class="nav-text">CenterNet: Objects as Points:</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hardnet"><span class="nav-number">2.</span> <span class="nav-text">HarDNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#densenet"><span class="nav-number">2.1.</span> <span class="nav-text">DenseNet</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#overview"><span class="nav-number">2.1.1.</span> <span class="nav-text">Overview</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#densenet-architecture"><span class="nav-number">2.1.2.</span> <span class="nav-text">DenseNet Architecture</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#densenet-b-bottleneck-layers"><span class="nav-number">2.1.3.</span> <span class="nav-text">DenseNet-B (Bottleneck Layers)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#result"><span class="nav-number">2.1.4.</span> <span class="nav-text">Result</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#downsides-of-previous-detection-algorithms"><span class="nav-number">2.2.</span> <span class="nav-text">Downsides of previous detection algorithms</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#focal-loss"><span class="nav-number">3.</span> <span class="nav-text">Focal Loss</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#object-detection-metrics"><span class="nav-number"></span> <span class="nav-text">Object Detection Metrics</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#intersection-over-union"><span class="nav-number">1.</span> <span class="nav-text">Intersection over Union</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#precision-and-recall"><span class="nav-number">2.</span> <span class="nav-text">Precision and Recall</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#precision-x-recall-curve-pr-curve"><span class="nav-number">3.</span> <span class="nav-text">Precision x Recall Curve (PR Curve)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#average-precision"><span class="nav-number">4.</span> <span class="nav-text">Average Precision</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#point-interpolation-method"><span class="nav-number">4.1.</span> <span class="nav-text">11-point interpolation method</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#all-point-interpolation-method"><span class="nav-number">4.2.</span> <span class="nav-text">All — point interpolation method</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mean-average-precision-map"><span class="nav-number">5.</span> <span class="nav-text">Mean Average Precision (mAP)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#map计算实例"><span class="nav-number">6.</span> <span class="nav-text">mAP计算实例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#point-interpolation"><span class="nav-number">6.1.</span> <span class="nav-text">11-point interpolation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#all-point-interpolation"><span class="nav-number">6.2.</span> <span class="nav-text">All-point interpolation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-coco-challenges-variants"><span class="nav-number">7.</span> <span class="nav-text">The COCO challenge’s variants</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#understanding-coco-dataset"><span class="nav-number">8.</span> <span class="nav-text">Understanding COCO dataset</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Nansong Yi"
      src="/images/baby.png">
  <p class="site-author-name" itemprop="name">Nansong Yi</p>
  <div class="site-description" itemprop="description">TheSong is named after IG.TheShy</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">14</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/nansongyi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;nansongyi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:nansong@uw.edu" title="E-Mail → mailto:nansong@uw.edu" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://nansongyi.me/" title="http:&#x2F;&#x2F;nansongyi.me" rel="noopener" target="_blank">my web</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://silencial.github.io/" title="https:&#x2F;&#x2F;silencial.github.io&#x2F;" rel="noopener" target="_blank">silencial</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://3monks.org/en/" title="https:&#x2F;&#x2F;3monks.org&#x2F;en&#x2F;" rel="noopener" target="_blank">sky</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.twistedwg.com/pages/class.html" title="http:&#x2F;&#x2F;www.twistedwg.com&#x2F;pages&#x2F;class.html" rel="noopener" target="_blank">twistedwg</a>
        </li>
    </ul>
  </div>

      </div>
      <!--
      <div style="position:absolute; bottom:120px left:auto; width:85%">
          <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=450 src="//music.163.com/outchain/player?type=0&id=5290225297&auto=1&height=430"></iframe>
      </div>
      -->

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Nansong Yi</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

    </div>
</body>
</html>
