<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/baby.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/baby.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/baby.png">
  <link rel="mask-icon" href="/images/baby.png" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Palatino:300,300italic,400,400italic,700,700italic|Monaco:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"thesong96.github.io","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"default"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="经典DQN  image-20210925021742794  DDQN with Prioritized Experience Replay  k是啥来着? k &#x3D; batch size?   image-20210925014701107  this blog is reproduced from here: Improving the Double DQN algorithm using p">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Reinforcement Learning">
<meta property="og:url" content="https://thesong96.github.io/2021/09/25/Deep-Reinforcement-Learning/index.html">
<meta property="og:site_name" content="TheSong">
<meta property="og:description" content="经典DQN  image-20210925021742794  DDQN with Prioritized Experience Replay  k是啥来着? k &#x3D; batch size?   image-20210925014701107  this blog is reproduced from here: Improving the Double DQN algorithm using p">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.imgur.com/dsI7w93.png">
<meta property="og:image" content="https://i.imgur.com/RLqXKoe.png">
<meta property="og:image" content="https://i.imgur.com/nFq13Mu.png">
<meta property="og:image" content="https://i.imgur.com/W9KbFXx.png">
<meta property="og:image" content="https://i.imgur.com/ObFtMi7.png">
<meta property="article:published_time" content="2021-09-25T08:46:39.000Z">
<meta property="article:modified_time" content="2021-10-14T07:17:26.056Z">
<meta property="article:author" content="Nansong Yi">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.imgur.com/dsI7w93.png">

<link rel="canonical" href="https://thesong96.github.io/2021/09/25/Deep-Reinforcement-Learning/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Deep Reinforcement Learning | TheSong</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/rss.xml" title="TheSong" type="application/rss+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">TheSong</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

      
    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://thesong96.github.io/2021/09/25/Deep-Reinforcement-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/baby.png">
      <meta itemprop="name" content="Nansong Yi">
      <meta itemprop="description" content="TheSong is named after IG.TheShy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TheSong">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Deep Reinforcement Learning
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-25 01:46:39" itemprop="dateCreated datePublished" datetime="2021-09-25T01:46:39-07:00">2021-09-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-10-14 00:17:26" itemprop="dateModified" datetime="2021-10-14T00:17:26-07:00">2021-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="经典dqn">经典DQN</h2>
<figure>
<img src="https://i.imgur.com/dsI7w93.png" alt="image-20210925021742794" /><figcaption aria-hidden="true">image-20210925021742794</figcaption>
</figure>
<h2 id="ddqn-with-prioritized-experience-replay">DDQN with Prioritized Experience Replay</h2>
<ul>
<li>k是啥来着? k = batch size?</li>
</ul>
<figure>
<img src="https://i.imgur.com/RLqXKoe.png" alt="image-20210925014701107" /><figcaption aria-hidden="true">image-20210925014701107</figcaption>
</figure>
<p>this blog is reproduced from <a href="https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/14/prioritized-experience-replay.html" target="_blank" rel="noopener">here</a>:</p>
<h1 id="improving-the-double-dqn-algorithm-using-prioritized-experience-replay">Improving the Double DQN algorithm using prioritized experience replay</h1>
<blockquote>
<p>Notes on improving the Double DQN algorithm using prioritized experience replay.</p>
</blockquote>
<ul>
<li>branch: 2020-04-14-prioritized-experience-replay</li>
<li>badges: true</li>
<li>image: images/prioritized-experience-replay.png</li>
<li>comments: true</li>
<li>author: David R. Pugh</li>
<li>categories: [pytorch, deep-reinforcement-learning, deep-q-networks]</li>
</ul>
<p>I am continuing to work my way through the <a href="https://www.udacity.com/" target="_blank" rel="noopener">Udacity</a> <a href="https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893" target="_blank" rel="noopener"><em>Deep Reinforcement Learning Nanodegree</em></a>. In this blog post I discuss and implement an important enhancement of the experience replay idea from <a href="https://arxiv.org/abs/1511.05952" target="_blank" rel="noopener"><em>Prioritized Experience Replay</em></a> (Schaul et al 2016).</p>
<p>The following quote from the paper nicely summarizes the key idea.</p>
<blockquote>
<p>Experience replay liberates online learning agents from processing transitions in the exact order they are experienced. Prioritized replay further liberates agents from considering transitions with the same frequency that they are experienced. ... In particular, we propose to more frequently replay transitions with high expected learning progress, as measured by the magnitude of their temporal-difference (TD) error. This prioritization can lead to a loss of diversity, which we alleviate with stochastic prioritization, and introduce bias, which we correct with importance sampling.</p>
</blockquote>
<p>Without further ado let's dive into discussing how to implement prioritized experience replay.</p>
<h2 id="prioritized-experience-replay">Prioritized Experience Replay</h2>
<p>Using an experience replay buffer naturally leads to two issues that need to be addressed.</p>
<ol type="1">
<li>Which experiences should the agent store in the replay buffer?</li>
<li>Which experiences should the agent replay from the buffer in order to learn efficiently?</li>
</ol>
<p>Schaul et al 2016 take the contents of the replay buffer more or less as given and focus solely on answering the second question. That is, the paper focuses on developing a procedure for making the most effective use of the experience replay buffer for learning.</p>
<p>Before discussing the procedure to sample from prioritized experiences, I need to discuss what information a reinforcement learning (RL) agent has available to prioritize its experiences for replay.</p>
<h3 id="prioritization-using-the-temporal-difference-td-error-term">Prioritization using the temporal-difference (TD) error term</h3>
<p>You can't prioritize experiences for learning unless you can measure the importance of each experience in the learning process. The ideal criterion would be the amount that RL agent can learn from experience given the current state (i.e., the expected learning value of the experience).</p>
<p>Unfortunately such an ideal criterion is not directly measurable. However, a reasonable proxy is the magnitude of an experience’s temporal-difference (TD) error <span class="math inline">\(\delta_i\)</span>. The TD-error indicates how "surprising" or "unexpected" the experience is given the current state of the RL agent. Using the TD-error term to prioritize experiences for replay is particularly suitable for incremental, online RL algorithms, such as <a href="https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" target="_blank" rel="noopener">SARSA</a> or <a href="https://en.wikipedia.org/wiki/Q-learning" target="_blank" rel="noopener">Q-learning</a>, as these algorithms already compute the TD-error and update the parameters proportionally.</p>
<p>Using the notation developed in my previous post on <a href="https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/11/double-dqn.html" target="_blank" rel="noopener"><em>Improving the DQN algorihtm using Double Q-learning</em></a> the TD-error term can be written as follows.</p>
<p><span class="math display">\[ \delta_{i,t} = R_{i,t+1} + \gamma Q\big(S_{i,t+1}, \underset{a}{\mathrm{argmax}}\ Q(S_{i,t+1}, a; \theta_t); \theta^{-}_t\big) - Q(S_{i,t}, a_{i,t}; \theta_t\big)\]</span></p>
<p>In the cell below I define a function for computing the TD-error (as well as several addition functions that will be used by the RL agent later in the post).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">synchronize_q_networks</span><span class="params">(q_network_1: nn.Module, q_network_2: nn.Module)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">    <span class="string">"""In place, synchronization of q_network_1 and q_network_2."""</span></span><br><span class="line">    _ = q_network_1.load_state_dict(q_network_2.state_dict())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_greedy_actions</span><span class="params">(states: torch.Tensor, q_network: nn.Module)</span> -&gt; torch.Tensor:</span></span><br><span class="line">    <span class="string">"""Select the greedy action for the current state given some Q-network."""</span></span><br><span class="line">    _, actions = q_network(states).max(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> actions</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_selected_actions</span><span class="params">(states: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                              actions: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                              rewards: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                              dones: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                              gamma: float,</span></span></span><br><span class="line"><span class="function"><span class="params">                              q_network: nn.Module)</span> -&gt; torch.Tensor:</span></span><br><span class="line">    <span class="string">"""Compute the Q-values by evaluating the actions given the current states and Q-network."""</span></span><br><span class="line">    next_q_values = q_network(states).gather(dim=<span class="number">1</span>, index=actions)        </span><br><span class="line">    q_values = rewards + (gamma * next_q_values * (<span class="number">1</span> - dones))</span><br><span class="line">    <span class="keyword">return</span> q_values</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">double_q_learning_update</span><span class="params">(states: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                             rewards: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                             dones: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                             gamma: float,</span></span></span><br><span class="line"><span class="function"><span class="params">                             q_network_1: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">                             q_network_2: nn.Module)</span> -&gt; torch.Tensor:</span></span><br><span class="line">    <span class="string">"""Double Q-Learning uses Q-network 1 to select actions and Q-network 2 to evaluate the selected actions."""</span></span><br><span class="line">    actions = select_greedy_actions(states, q_network_1)</span><br><span class="line">    q_values = evaluate_selected_actions(states, actions, rewards, dones, gamma, q_network_2)</span><br><span class="line">    <span class="keyword">return</span> q_values</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">double_q_learning_error</span><span class="params">(states: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                            actions: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                            rewards: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                            next_states: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                            dones: torch.Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                            gamma: float,</span></span></span><br><span class="line"><span class="function"><span class="params">                            q_network_1: nn.Module,</span></span></span><br><span class="line"><span class="function"><span class="params">                            q_network_2: nn.Module)</span> -&gt; torch.Tensor:</span></span><br><span class="line">    expected_q_values = double_q_learning_update(next_states, rewards, dones, gamma, q_network_1, q_network_2)</span><br><span class="line">    q_values = q_network_1(states).gather(dim=<span class="number">1</span>, index=actions)</span><br><span class="line">    delta = expected_q_values - q_values</span><br><span class="line">    <span class="keyword">return</span> delta</span><br></pre></td></tr></table></figure>
<p>Now that I have defined a measurable criterion by which an RL agent can prioritize its experiences, I can move on to discussing the major contribution of the Schaul et al 2016 paper which was an efficient procedure for randomly sampling and replaying prioritized experiences.</p>
<h3 id="stochastic-prioritization">Stochastic prioritization</h3>
<p>Schaul et al 2016 introduce a stochastic sampling method that interpolates between pure greedy experience prioritization (i.e., always sampling the highest priority experiences) and uniform random sampling of experience. The probability of sampling experience <span class="math inline">\(i\)</span> is defined as follows.</p>
<p><span class="math display">\[ P(i) = \frac{p_i^{\alpha}}{\sum_{j=0}^{N} p_j^{\alpha}} \]</span></p>
<p>where <span class="math inline">\(p_i &gt; 0\)</span> is the priority of transition <span class="math inline">\(i\)</span>. The exponent <span class="math inline">\(\alpha\)</span> determines how much prioritization is used, with <span class="math inline">\(\alpha = 0\)</span> corresponding to the uniform random sampling case. Note that the probability of being sampled is monotonic in an experience’s priority while guaranteeing a non-zero probability for the lowest-priority experience.</p>
<h3 id="correcting-for-sampling-bias">Correcting for sampling bias</h3>
<p>Estimation of the expected value from stochastic updates relies on those updates being drawn from the same underlying distribution whose expectation you wish to estimate. Prioritized experience replay introduces a form of sampling bias that changes the underlying distribution (whose expectation needs to be estimated) in an uncontrolled fashion. When the underlying distribution changes, the solution to which the algorithm will converge also changes (even if the policy and state distribution are fixed). In order for the algorithm to converge properly, the bias introduced by the prioritized experience replay procedure needs to be corrected.</p>
<p>Schaul et al 2016 correct for this bias using an importance sampling scheme that computes a weight for each sampled experience that can be used when computing the loss for that sample.</p>
<p><span class="math display">\[ w_i = \left(\frac{1}{N}\frac{1}{P(i)}\right)^\beta \]</span></p>
<p>The hyperparameter <span class="math inline">\(\beta \ge 0\)</span> controls how strongly to correct for the bias: <span class="math inline">\(\beta=0\)</span> implies no correction; <span class="math inline">\(\beta=1\)</span> fully compensates for the bias. For stability reasons, since these importance sampling weights are included in the loss, they are be normalized by <span class="math inline">\(\max_i\ w_i\)</span>.</p>
<h3 id="implementation">Implementation</h3>
<p>The <code>PrioritizedExperienceReplayBuffer</code> defined below is a substantial re-write of the <code>ExperienceReplayBuffer</code> that I used in my <a href="https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/03/deep-q-networks.html" target="_blank" rel="noopener">previous</a> <a href="https://davidrpugh.github.io/stochastic-expatriate-descent/pytorch/deep-reinforcement-learning/deep-q-networks/2020/04/11/double-dqn.html" target="_blank" rel="noopener">posts</a>.</p>
<p>The most important implementation detail is that instead of using a <a href="https://docs.python.org/3.8/library/collections.html#collections.deque" target="_blank" rel="noopener">fixed-length, double-ended queue</a> as the underlying data structure for storing experiences, I am now using a NumPy <a href="https://docs.scipy.org/doc/numpy/user/basics.rec.html" target="_blank" rel="noopener">structured array</a> to store priority-experience tuples. In addition to cleaning up a lot of the internal implementation details, using a structured array as an internal buffer led to significant performance improvements.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> typing</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">_field_names = [</span><br><span class="line">    <span class="string">"state"</span>,</span><br><span class="line">    <span class="string">"action"</span>,</span><br><span class="line">    <span class="string">"reward"</span>,</span><br><span class="line">    <span class="string">"next_state"</span>,</span><br><span class="line">    <span class="string">"done"</span></span><br><span class="line">]</span><br><span class="line">Experience = collections.namedtuple(<span class="string">"Experience"</span>, field_names=_field_names)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PrioritizedExperienceReplayBuffer</span>:</span></span><br><span class="line">    <span class="string">"""Fixed-size buffer to store priority, Experience tuples."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 batch_size: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 buffer_size: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 alpha: float = <span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 random_state: np.random.RandomState = None)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Initialize an ExperienceReplayBuffer object.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">        -----------</span></span><br><span class="line"><span class="string">        buffer_size (int): maximum size of buffer</span></span><br><span class="line"><span class="string">        batch_size (int): size of each training batch</span></span><br><span class="line"><span class="string">        alpha (float): Strength of prioritized sampling. Default to 0.0 (i.e., uniform sampling).</span></span><br><span class="line"><span class="string">        random_state (np.random.RandomState): random number generator.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self._batch_size = batch_size</span><br><span class="line">        self._buffer_size = buffer_size</span><br><span class="line">        self._buffer_length = <span class="number">0</span> <span class="comment"># current number of prioritized experience tuples in buffer</span></span><br><span class="line">        self._buffer = np.empty(self._buffer_size, dtype=[(<span class="string">"priority"</span>, np.float32), (<span class="string">"experience"</span>, Experience)])</span><br><span class="line">        self._alpha = alpha</span><br><span class="line">        self._random_state = np.random.RandomState() <span class="keyword">if</span> random_state <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> random_state</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span> -&gt; int:</span></span><br><span class="line">        <span class="string">"""Current number of prioritized experience tuple stored in buffer."""</span></span><br><span class="line">        <span class="keyword">return</span> self._buffer_length</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">alpha</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Strength of prioritized sampling."""</span></span><br><span class="line">        <span class="keyword">return</span> self._alpha</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">batch_size</span><span class="params">(self)</span> -&gt; int:</span></span><br><span class="line">        <span class="string">"""Number of experience samples per training batch."""</span></span><br><span class="line">        <span class="keyword">return</span> self._batch_size</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">buffer_size</span><span class="params">(self)</span> -&gt; int:</span></span><br><span class="line">        <span class="string">"""Maximum number of prioritized experience tuples stored in buffer."""</span></span><br><span class="line">        <span class="keyword">return</span> self._buffer_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(self, experience: Experience)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="string">"""Add a new experience to memory."""</span></span><br><span class="line">        priority = <span class="number">1.0</span> <span class="keyword">if</span> self.is_empty() <span class="keyword">else</span> self._buffer[<span class="string">"priority"</span>].max()</span><br><span class="line">        <span class="keyword">if</span> self.is_full():</span><br><span class="line">            <span class="keyword">if</span> priority &gt; self._buffer[<span class="string">"priority"</span>].min():</span><br><span class="line">                idx = self._buffer[<span class="string">"priority"</span>].argmin()</span><br><span class="line">                self._buffer[idx] = (priority, experience)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">pass</span> <span class="comment"># low priority experiences should not be included in buffer</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self._buffer[self._buffer_length] = (priority, experience)</span><br><span class="line">            self._buffer_length += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_empty</span><span class="params">(self)</span> -&gt; bool:</span></span><br><span class="line">        <span class="string">"""True if the buffer is empty; False otherwise."""</span></span><br><span class="line">        <span class="keyword">return</span> self._buffer_length == <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_full</span><span class="params">(self)</span> -&gt; bool:</span></span><br><span class="line">        <span class="string">"""True if the buffer is full; False otherwise."""</span></span><br><span class="line">        <span class="keyword">return</span> self._buffer_length == self._buffer_size</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(self, beta: float)</span> -&gt; typing.Tuple[np.array, np.array, np.array]:</span></span><br><span class="line">        <span class="string">"""Sample a batch of experiences from memory."""</span></span><br><span class="line">        <span class="comment"># use sampling scheme to determine which experiences to use for learning</span></span><br><span class="line">        ps = self._buffer[:self._buffer_length][<span class="string">"priority"</span>]</span><br><span class="line">        sampling_probs = ps**self._alpha / np.sum(ps**self._alpha)</span><br><span class="line">        idxs = self._random_state.choice(np.arange(ps.size),</span><br><span class="line">                                         size=self._batch_size,</span><br><span class="line">                                         replace=<span class="literal">True</span>,</span><br><span class="line">                                         p=sampling_probs)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># select the experiences and compute sampling weights</span></span><br><span class="line">        experiences = self._buffer[<span class="string">"experience"</span>][idxs]        </span><br><span class="line">        weights = (self._buffer_length * sampling_probs[idxs])**-beta</span><br><span class="line">        normalized_weights = weights / weights.max()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> idxs, experiences, normalized_weights</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_priorities</span><span class="params">(self, idxs: np.array, priorities: np.array)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="string">"""Update the priorities associated with particular experiences."""</span></span><br><span class="line">        self._buffer[<span class="string">"priority"</span>][idxs] = priorities</span><br></pre></td></tr></table></figure>
<h2 id="refactoring-the-deepqagent-class">Refactoring the <code>DeepQAgent</code> class</h2>
<p>Other than continuing to clean up internal implementation details, nothing really changed from the implementation of the <code>DeepQAgent</code> from my previous posts. I added two additional parameters to the constructor: <code>alpha</code> which controls the strength of the prioritization sampling and <code>beta_annealing_schedule</code> (discussed in detail below) which allows the strength of the sampling bias correction (i.e., the importance sampling weights) to increase as training progresses.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> typing</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">A = typing.TypeVar(<span class="string">'A'</span>, bound=<span class="string">'Agent'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Agent</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span><span class="params">(self, state: np.array)</span> -&gt; int:</span></span><br><span class="line">        <span class="string">"""Rule for choosing an action given the current state of the environment."""</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">(self, experiences: typing.List[Experience])</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="string">"""Update the agent's state based on a collection of recent experiences."""</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self, filepath)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="string">"""Save any important agent state to a file."""</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">             state: np.array,</span></span></span><br><span class="line"><span class="function"><span class="params">             action: int,</span></span></span><br><span class="line"><span class="function"><span class="params">             reward: float,</span></span></span><br><span class="line"><span class="function"><span class="params">             next_state: np.array,</span></span></span><br><span class="line"><span class="function"><span class="params">             done: bool)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="string">"""Update agent's state after observing the effect of its action on the environment."""</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplmentedError</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepQAgent</span><span class="params">(Agent)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 state_size: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 action_size: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 number_hidden_units: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 optimizer_fn: typing.Callable[[typing.Iterable[nn.Parameter]], optim.Optimizer],</span></span></span><br><span class="line"><span class="function"><span class="params">                 batch_size: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 buffer_size: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 alpha: float,</span></span></span><br><span class="line"><span class="function"><span class="params">                 beta_annealing_schedule: typing.Callable[[int], float],</span></span></span><br><span class="line"><span class="function"><span class="params">                 epsilon_decay_schedule: typing.Callable[[int], float],</span></span></span><br><span class="line"><span class="function"><span class="params">                 gamma: float,</span></span></span><br><span class="line"><span class="function"><span class="params">                 update_frequency: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 seed: int = None)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Initialize a DeepQAgent.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">        -----------</span></span><br><span class="line"><span class="string">        state_size (int): the size of the state space.</span></span><br><span class="line"><span class="string">        action_size (int): the size of the action space.</span></span><br><span class="line"><span class="string">        number_hidden_units (int): number of units in the hidden layers.</span></span><br><span class="line"><span class="string">        optimizer_fn (callable): function that takes Q-network parameters and returns an optimizer.</span></span><br><span class="line"><span class="string">        batch_size (int): number of experience tuples in each mini-batch.</span></span><br><span class="line"><span class="string">        buffer_size (int): maximum number of experience tuples stored in the replay buffer.</span></span><br><span class="line"><span class="string">        alpha (float): Strength of prioritized sampling; alpha &gt;= 0.0.</span></span><br><span class="line"><span class="string">        beta_annealing_schedule (callable): function that takes episode number and returns beta &gt;= 0.</span></span><br><span class="line"><span class="string">        epsilon_decay_schdule (callable): function that takes episode number and returns 0 &lt;= epsilon &lt; 1.</span></span><br><span class="line"><span class="string">        alpha (float): rate at which the target q-network parameters are updated.</span></span><br><span class="line"><span class="string">        gamma (float): Controls how much that agent discounts future rewards (0 &lt; gamma &lt;= 1).</span></span><br><span class="line"><span class="string">        update_frequency (int): frequency (measured in time steps) with which q-network parameters are updated.</span></span><br><span class="line"><span class="string">        seed (int): random seed</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self._state_size = state_size</span><br><span class="line">        self._action_size = action_size</span><br><span class="line">        self._device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># set seeds for reproducibility</span></span><br><span class="line">        self._random_state = np.random.RandomState() <span class="keyword">if</span> seed <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> np.random.RandomState(seed)</span><br><span class="line">        <span class="keyword">if</span> seed <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            torch.manual_seed(seed)</span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">            torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># initialize agent hyperparameters</span></span><br><span class="line">        _replay_buffer_kwargs = &#123;</span><br><span class="line">            <span class="string">"alpha"</span>: alpha,</span><br><span class="line">            <span class="string">"batch_size"</span>: batch_size,</span><br><span class="line">            <span class="string">"buffer_size"</span>: buffer_size,</span><br><span class="line">            <span class="string">"random_state"</span>: self._random_state</span><br><span class="line">        &#125;</span><br><span class="line">        self._memory = PrioritizedExperienceReplayBuffer(**_replay_buffer_kwargs)</span><br><span class="line">        self._beta_annealing_schedule = beta_annealing_schedule</span><br><span class="line">        self._epsilon_decay_schedule = epsilon_decay_schedule</span><br><span class="line">        self._gamma = gamma</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># initialize Q-Networks</span></span><br><span class="line">        self._update_frequency = update_frequency</span><br><span class="line">        self._online_q_network = self._initialize_q_network(number_hidden_units)</span><br><span class="line">        self._target_q_network = self._initialize_q_network(number_hidden_units)</span><br><span class="line">        synchronize_q_networks(self._target_q_network, self._online_q_network)        </span><br><span class="line">        self._online_q_network.to(self._device)</span><br><span class="line">        self._target_q_network.to(self._device)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># initialize the optimizer</span></span><br><span class="line">        self._optimizer = optimizer_fn(self._online_q_network.parameters())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># initialize some counters</span></span><br><span class="line">        self._number_episodes = <span class="number">0</span></span><br><span class="line">        self._number_timesteps = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_initialize_q_network</span><span class="params">(self, number_hidden_units: int)</span> -&gt; nn.Module:</span></span><br><span class="line">        <span class="string">"""Create a neural network for approximating the action-value function."""</span></span><br><span class="line">        q_network = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=self._state_size, out_features=number_hidden_units),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=number_hidden_units, out_features=number_hidden_units),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=number_hidden_units, out_features=self._action_size)</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> q_network</span><br><span class="line">           </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_uniform_random_policy</span><span class="params">(self, state: torch.Tensor)</span> -&gt; int:</span></span><br><span class="line">        <span class="string">"""Choose an action uniformly at random."""</span></span><br><span class="line">        <span class="keyword">return</span> self._random_state.randint(self._action_size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_greedy_policy</span><span class="params">(self, state: torch.Tensor)</span> -&gt; int:</span></span><br><span class="line">        <span class="string">"""Choose an action that maximizes the action_values given the current state."""</span></span><br><span class="line">        actions = select_greedy_actions(state, self._online_q_network)</span><br><span class="line">        action = (actions.cpu()  <span class="comment"># actions might reside on the GPU!</span></span><br><span class="line">                         .item())</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_epsilon_greedy_policy</span><span class="params">(self, state: torch.Tensor, epsilon: float)</span> -&gt; int:</span></span><br><span class="line">        <span class="string">"""With probability epsilon explore randomly; otherwise exploit knowledge optimally."""</span></span><br><span class="line">        <span class="keyword">if</span> self._random_state.random() &lt; epsilon:</span><br><span class="line">            action = self._uniform_random_policy(state)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = self._greedy_policy(state)</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span><span class="params">(self, state: np.array)</span> -&gt; int:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Return the action for given state as per current policy.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">        -----------</span></span><br><span class="line"><span class="string">        state (np.array): current state of the environment.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Return:</span></span><br><span class="line"><span class="string">        --------</span></span><br><span class="line"><span class="string">        action (int): an integer representing the chosen action.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># need to reshape state array and convert to tensor</span></span><br><span class="line">        state_tensor = (torch.from_numpy(state)</span><br><span class="line">                             .unsqueeze(dim=<span class="number">0</span>)</span><br><span class="line">                             .to(self._device))</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># choose uniform at random if agent has insufficient experience</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.has_sufficient_experience():</span><br><span class="line">            action = self._uniform_random_policy(state_tensor)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            epsilon = self._epsilon_decay_schedule(self._number_episodes)</span><br><span class="line">            action = self._epsilon_greedy_policy(state_tensor, epsilon)</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">(self, idxs: np.array, experiences: np.array, sampling_weights: np.array)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="string">"""Update the agent's state based on a collection of recent experiences."""</span></span><br><span class="line">        states, actions, rewards, next_states, dones = (torch.Tensor(vs).to(self._device) <span class="keyword">for</span> vs <span class="keyword">in</span> zip(*experiences))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># need to add second dimension to some tensors</span></span><br><span class="line">        actions = (actions.long()</span><br><span class="line">                          .unsqueeze(dim=<span class="number">1</span>))</span><br><span class="line">        rewards = rewards.unsqueeze(dim=<span class="number">1</span>)</span><br><span class="line">        dones = dones.unsqueeze(dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        deltas = double_q_learning_error(states,</span><br><span class="line">                                         actions,</span><br><span class="line">                                         rewards,</span><br><span class="line">                                         next_states,</span><br><span class="line">                                         dones,</span><br><span class="line">                                         self._gamma,</span><br><span class="line">                                         self._online_q_network,</span><br><span class="line">                                         self._target_q_network)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># update experience priorities</span></span><br><span class="line">        priorities = (deltas.abs()</span><br><span class="line">                            .cpu()</span><br><span class="line">                            .detach()</span><br><span class="line">                            .numpy()</span><br><span class="line">                            .flatten())</span><br><span class="line">        self._memory.update_priorities(idxs, priorities + <span class="number">1e-6</span>) <span class="comment"># priorities must be positive!</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># compute the mean squared loss</span></span><br><span class="line">        _sampling_weights = (torch.Tensor(sampling_weights)</span><br><span class="line">                                  .view((<span class="number">-1</span>, <span class="number">1</span>)))</span><br><span class="line">        loss = torch.mean((deltas * _sampling_weights)**<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># updates the parameters of the online network</span></span><br><span class="line">        self._optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        self._optimizer.step()</span><br><span class="line">        </span><br><span class="line">        synchronize_q_networks(self._target_q_network, self._online_q_network)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">has_sufficient_experience</span><span class="params">(self)</span> -&gt; bool:</span></span><br><span class="line">        <span class="string">"""True if agent has enough experience to train on a batch of samples; False otherwise."""</span></span><br><span class="line">        <span class="keyword">return</span> len(self._memory) &gt;= self._memory.batch_size</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self, filepath: str)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Saves the state of the DeepQAgent.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">        -----------</span></span><br><span class="line"><span class="string">        filepath (str): filepath where the serialized state should be saved.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Notes:</span></span><br><span class="line"><span class="string">        ------</span></span><br><span class="line"><span class="string">        The method uses `torch.save` to serialize the state of the q-network, </span></span><br><span class="line"><span class="string">        the optimizer, as well as the dictionary of agent hyperparameters.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        checkpoint = &#123;</span><br><span class="line">            <span class="string">"q-network-state"</span>: self._online_q_network.state_dict(),</span><br><span class="line">            <span class="string">"optimizer-state"</span>: self._optimizer.state_dict(),</span><br><span class="line">            <span class="string">"agent-hyperparameters"</span>: &#123;</span><br><span class="line">                <span class="string">"alpha"</span>: self._memory.alpha,</span><br><span class="line">                <span class="string">"beta_annealing_schedule"</span>: self._beta_annealing_schedule,</span><br><span class="line">                <span class="string">"batch_size"</span>: self._memory.batch_size,</span><br><span class="line">                <span class="string">"buffer_size"</span>: self._memory.buffer_size,</span><br><span class="line">                <span class="string">"epsilon_decay_schedule"</span>: self._epsilon_decay_schedule,</span><br><span class="line">                <span class="string">"gamma"</span>: self._gamma,</span><br><span class="line">                <span class="string">"update_frequency"</span>: self._update_frequency</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        torch.save(checkpoint, filepath)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">             state: np.array,</span></span></span><br><span class="line"><span class="function"><span class="params">             action: int,</span></span></span><br><span class="line"><span class="function"><span class="params">             reward: float,</span></span></span><br><span class="line"><span class="function"><span class="params">             next_state: np.array,</span></span></span><br><span class="line"><span class="function"><span class="params">             done: bool)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Updates the agent's state based on feedback received from the environment.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">        -----------</span></span><br><span class="line"><span class="string">        state (np.array): the previous state of the environment.</span></span><br><span class="line"><span class="string">        action (int): the action taken by the agent in the previous state.</span></span><br><span class="line"><span class="string">        reward (float): the reward received from the environment.</span></span><br><span class="line"><span class="string">        next_state (np.array): the resulting state of the environment following the action.</span></span><br><span class="line"><span class="string">        done (bool): True is the training episode is finised; false otherwise.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        experience = Experience(state, action, reward, next_state, done)</span><br><span class="line">        self._memory.add(experience) </span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            self._number_episodes += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self._number_timesteps += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># every so often the agent should learn from experiences</span></span><br><span class="line">            <span class="keyword">if</span> self._number_timesteps % self._update_frequency == <span class="number">0</span> <span class="keyword">and</span> self.has_sufficient_experience():</span><br><span class="line">                beta = self._beta_annealing_schedule(self._number_episodes)</span><br><span class="line">                idxs, experiences, sampling_weights = self._memory.sample(beta)</span><br><span class="line">                self.learn(idxs, experiences, sampling_weights)</span><br></pre></td></tr></table></figure>
<h2 id="the-training-loop">The Training Loop</h2>
<p>The code for the training loop remains unchanged from previous posts.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> typing</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_train_for_at_most</span><span class="params">(agent: Agent, env: gym.Env, max_timesteps: int)</span> -&gt; int:</span></span><br><span class="line">    <span class="string">"""Train agent for a maximum number of timesteps."""</span></span><br><span class="line">    state = env.reset()</span><br><span class="line">    score = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(max_timesteps):</span><br><span class="line">        action = agent.choose_action(state)</span><br><span class="line">        next_state, reward, done, _ = env.step(action)</span><br><span class="line">        agent.step(state, action, reward, next_state, done)</span><br><span class="line">        state = next_state</span><br><span class="line">        score += reward</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line">                </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_train_until_done</span><span class="params">(agent: Agent, env: gym.Env)</span> -&gt; float:</span></span><br><span class="line">    <span class="string">"""Train the agent until the current episode is complete."""</span></span><br><span class="line">    state = env.reset()</span><br><span class="line">    score = <span class="number">0</span></span><br><span class="line">    done = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">        action = agent.choose_action(state)</span><br><span class="line">        next_state, reward, done, _ = env.step(action)</span><br><span class="line">        agent.step(state, action, reward, next_state, done)</span><br><span class="line">        state = next_state</span><br><span class="line">        score += reward</span><br><span class="line">    <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(agent: Agent,</span></span></span><br><span class="line"><span class="function"><span class="params">          env: gym.Env,</span></span></span><br><span class="line"><span class="function"><span class="params">          checkpoint_filepath: str,</span></span></span><br><span class="line"><span class="function"><span class="params">          target_score: float,</span></span></span><br><span class="line"><span class="function"><span class="params">          number_episodes: int,</span></span></span><br><span class="line"><span class="function"><span class="params">          maximum_timesteps=None)</span> -&gt; typing.List[float]:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Reinforcement learning training loop.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">    -----------</span></span><br><span class="line"><span class="string">    agent (Agent): an agent to train.</span></span><br><span class="line"><span class="string">    env (gym.Env): an environment in which to train the agent.</span></span><br><span class="line"><span class="string">    checkpoint_filepath (str): filepath used to save the state of the trained agent.</span></span><br><span class="line"><span class="string">    number_episodes (int): maximum number of training episodes.</span></span><br><span class="line"><span class="string">    maximum_timesteps (int): maximum number of timesteps per episode.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    --------</span></span><br><span class="line"><span class="string">    scores (list): collection of episode scores from training.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    scores = []</span><br><span class="line">    most_recent_scores = collections.deque(maxlen=<span class="number">100</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(number_episodes):</span><br><span class="line">        <span class="keyword">if</span> maximum_timesteps <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            score = _train_until_done(agent, env)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            score = _train_for_at_most(agent, env, maximum_timesteps)         </span><br><span class="line">        scores.append(score)</span><br><span class="line">        most_recent_scores.append(score)</span><br><span class="line">        </span><br><span class="line">        average_score = sum(most_recent_scores) / len(most_recent_scores)</span><br><span class="line">        <span class="keyword">if</span> average_score &gt;= target_score:</span><br><span class="line">            print(<span class="string">f"\nEnvironment solved in <span class="subst">&#123;i:d&#125;</span> episodes!\tAverage Score: <span class="subst">&#123;average_score:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line">            agent.save(checkpoint_filepath)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">f"\rEpisode <span class="subst">&#123;i + <span class="number">1</span>&#125;</span>\tAverage Score: <span class="subst">&#123;average_score:<span class="number">.2</span>f&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> scores</span><br></pre></td></tr></table></figure>
<h2 id="solving-the-lunarlander-v2-environment">Solving the <code>LunarLander-v2</code> environment</h2>
<p>In the rest of this blog post I will use the Double DQN algorithm with prioritized experience replay to train an agent to solve the <a href="https://gym.openai.com/envs/LunarLander-v2/" target="_blank" rel="noopener">LunarLander-v2</a> environment from <a href="https://openai.com/" target="_blank" rel="noopener">OpenAI</a>.</p>
<p>In this environment the landing pad is always at coordinates (0,0). The reward for moving the lander from the top of the screen to landing pad and arriving at zero speed is typically between 100 and 140 points. Firing the main engine is -0.3 points each frame (so the lander is incentivized to fire the engine as few times possible). If the lander moves away from landing pad it loses reward (so the lander is incentived to land in the designated landing area). The lander is also incentived to land "gracefully" (and not crash in the landing area!).</p>
<p>A training episode finishes if the lander crashes (-100 points) or comes to rest (+100 points). Each leg with ground contact receives and additional +10 points. The task is considered "solved" if the lander is able to achieve 200 points (I will actually be more stringent and define "solved" as achieving over 200 points on average in the most recent 100 training episodes).</p>
<h3 id="action-space">Action Space</h3>
<p>There are four discrete actions available:</p>
<ol start="0" type="1">
<li>Do nothing.</li>
<li>Fire the left orientation engine.</li>
<li>Fire main engine.</li>
<li>Fire the right orientation engine.</li>
</ol>
<h3 id="colab-specific-environment-setup">Colab specific environment setup</h3>
<p>If you are playing around with this notebook on Google Colab, then you will need to run the following cell in order to install the required OpenAI dependencies into the environment.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install gym[box2d]==<span class="number">0.17</span>.*</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">'LunarLander-v2'</span>)</span><br><span class="line">_ = env.seed(<span class="number">42</span>)</span><br></pre></td></tr></table></figure>
<pre><code>/Users/pughdr/Research/stochastic-expatriate-descent/env/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize(&#39;%s: %s&#39;%(&#39;WARN&#39;, msg % args), &#39;yellow&#39;))</code></pre>
<h3 id="creating-a-deepqagent">Creating a <code>DeepQAgent</code></h3>
<p>Before creating an instance of the <code>DeepQAgent</code> with prioritized experience replay I need to define a <span class="math inline">\(\beta\)</span>-annealing schedule, an <span class="math inline">\(\epsilon\)</span>-decay schedule, and choose an optimizer.</p>
<h4 id="beta-annealing-schedule"><span class="math inline">\(\beta\)</span>-annealing schedule</h4>
<p>Due to the inherent non-stationarity of the RL training process, Schaul et al 2016 hypothesize that a small sampling bias can be ignored during early training episodes. Instead of fixing <span class="math inline">\(\beta=1\)</span> (and fully correcting for the bias throughout training) they increase the amount of importance sampling correction as the number of training episodes increase by defining a schedule for <span class="math inline">\(\beta\)</span> that reaches 1 (i.e., full bias correction) only near the end of training.</p>
<p>Note that the choice of <span class="math inline">\(\beta\)</span> interacts with choice of prioritization exponent <span class="math inline">\(\alpha\)</span>: increasing both simultaneously prioritizes sampling more aggressively while at the same time as correcting for it more strongly.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">constant_annealing_schedule</span><span class="params">(n, constant)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> constant</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exponential_annealing_schedule</span><span class="params">(n, rate)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - np.exp(-rate * n)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">1</span>, figsize=(<span class="number">10</span>,<span class="number">6</span>))</span><br><span class="line">ns = np.arange(<span class="number">2000</span>)</span><br><span class="line">rate = <span class="number">1e-2</span></span><br><span class="line">_ = ax.plot(ns, exponential_annealing_schedule(ns, rate))</span><br><span class="line">_ = ax.set_ylabel(<span class="string">r"$\beta$"</span>, rotation=<span class="string">"horizontal"</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">_ = ax.set_xlabel(<span class="string">"Number of Episodes"</span>, fontsize=<span class="number">15</span>)</span><br><span class="line">_ = ax.set_title(<span class="string">r"Typical $\beta$ annealing schedule"</span>, fontsize=<span class="number">15</span>)</span><br></pre></td></tr></table></figure>
<p>​<br />
<img src="https://i.imgur.com/nFq13Mu.png" alt="png" /> ​</p>
<h4 id="epsilon-decay-schedule"><span class="math inline">\(\epsilon\)</span>-decay schedule</h4>
<p>As was the case with the DQN and Double DQN algorithms, the agent chooses its action using an <span class="math inline">\(\epsilon\)</span>-greedy policy. When using an <span class="math inline">\(\epsilon\)</span>-greedy policy, with probability <span class="math inline">\(\epsilon\)</span>, the agent explores the state space by choosing an action uniformly at random from the set of feasible actions; with probability <span class="math inline">\(1-\epsilon\)</span>, the agent exploits its current knowledge by choosing the optimal action given that current state.</p>
<p>As the agent learns and acquires additional knowledge about it environment it makes sense to <em>decrease</em> exploration and <em>increase</em> exploitation by decreasing <span class="math inline">\(\epsilon\)</span>. In practice, it isn't a good idea to decrease <span class="math inline">\(\epsilon\)</span> to zero; instead one typically decreases <span class="math inline">\(\epsilon\)</span> over time according to some schedule until it reaches some minimum value.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">power_decay_schedule</span><span class="params">(episode_number: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                         decay_factor: float,</span></span></span><br><span class="line"><span class="function"><span class="params">                         minimum_epsilon: float)</span> -&gt; float:</span></span><br><span class="line">    <span class="string">"""Power decay schedule found in other practical applications."""</span></span><br><span class="line">    <span class="keyword">return</span> max(decay_factor**episode_number, minimum_epsilon)</span><br><span class="line"></span><br><span class="line">_epsilon_decay_schedule_kwargs = &#123;</span><br><span class="line">    <span class="string">"decay_factor"</span>: <span class="number">0.99</span>,</span><br><span class="line">    <span class="string">"minimum_epsilon"</span>: <span class="number">1e-2</span>,</span><br><span class="line">&#125;</span><br><span class="line">epsilon_decay_schedule = <span class="keyword">lambda</span> n: power_decay_schedule(n, **_epsilon_decay_schedule_kwargs)</span><br></pre></td></tr></table></figure>
<h4 id="choosing-an-optimizer">Choosing an optimizer</h4>
<p>Given the good results I achieved in my previous post using the <a href="https://pytorch.org/docs/stable/optim.html#torch.optim.Adam" target="_blank" rel="noopener">Adam</a> optimizer I decided to continue to use that optimizer here.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">_optimizer_kwargs = &#123;</span><br><span class="line">    <span class="string">"lr"</span>: <span class="number">1e-3</span>,</span><br><span class="line">    <span class="string">"betas"</span>:(<span class="number">0.9</span>, <span class="number">0.999</span>),</span><br><span class="line">    <span class="string">"eps"</span>: <span class="number">1e-08</span>,</span><br><span class="line">    <span class="string">"weight_decay"</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">"amsgrad"</span>: <span class="literal">False</span>,</span><br><span class="line">&#125;</span><br><span class="line">optimizer_fn = <span class="keyword">lambda</span> parameters: optim.Adam(parameters, **_optimizer_kwargs)</span><br></pre></td></tr></table></figure>
<h3 id="training-the-deepqagent">Training the <code>DeepQAgent</code></h3>
<p>Now I am finally ready to train the <code>deep_q_agent</code>. The target score for the <code>LunarLander-v2</code> environment is 200 points on average for at least 100 consecutive episodes. First, I will train an RL agent with <span class="math inline">\(\alpha=0.0\)</span> and <span class="math inline">\(\beta=0\)</span> (throught training) which will recover the uniform random sampling baseline. Then I will re-train the RL agent using prioritized sampling for comparison.</p>
<h4 id="uniform-random-sampling">Uniform random sampling</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">_agent_kwargs = &#123;</span><br><span class="line">    <span class="string">"state_size"</span>: env.observation_space.shape[<span class="number">0</span>],</span><br><span class="line">    <span class="string">"action_size"</span>: env.action_space.n, </span><br><span class="line">    <span class="string">"number_hidden_units"</span>: <span class="number">64</span>,</span><br><span class="line">    <span class="string">"optimizer_fn"</span>: optimizer_fn,</span><br><span class="line">    <span class="string">"epsilon_decay_schedule"</span>: epsilon_decay_schedule,</span><br><span class="line">    <span class="string">"alpha"</span>: <span class="number">0.0</span>,</span><br><span class="line">    <span class="string">"batch_size"</span>: <span class="number">64</span>,</span><br><span class="line">    <span class="string">"buffer_size"</span>: <span class="number">100000</span>,</span><br><span class="line">    <span class="string">"beta_annealing_schedule"</span>: <span class="keyword">lambda</span> n: constant_annealing_schedule(n, <span class="number">0.0</span>),</span><br><span class="line">    <span class="string">"gamma"</span>: <span class="number">0.99</span>,</span><br><span class="line">    <span class="string">"update_frequency"</span>: <span class="number">4</span>,</span><br><span class="line">    <span class="string">"seed"</span>: <span class="literal">None</span>,</span><br><span class="line">&#125;</span><br><span class="line">double_dqn_agent = DeepQAgent(**_agent_kwargs)</span><br><span class="line"></span><br><span class="line">uniform_sampling_scores = train(double_dqn_agent,</span><br><span class="line">                                env,</span><br><span class="line">                                <span class="string">"uniform-sampling-checkpoint.pth"</span>,</span><br><span class="line">                                number_episodes=<span class="number">2000</span>,</span><br><span class="line">                                target_score=float(<span class="string">"inf"</span>))</span><br></pre></td></tr></table></figure>
<pre><code>Episode 100 Average Score: -113.36
Episode 200 Average Score: 51.15
Episode 300 Average Score: 119.37
Episode 400 Average Score: 158.45
Episode 500 Average Score: 154.08
Episode 600 Average Score: 216.22
Episode 700 Average Score: 220.47
Episode 800 Average Score: 229.12
Episode 900 Average Score: 223.13
Episode 1000    Average Score: 230.66
Episode 1100    Average Score: 229.96
Episode 1200    Average Score: 209.60
Episode 1300    Average Score: 190.09
Episode 1400    Average Score: 212.46
Episode 1500    Average Score: 219.84
Episode 1600    Average Score: 226.77
Episode 1700    Average Score: 231.39
Episode 1800    Average Score: 208.05
Episode 1900    Average Score: 183.24
Episode 2000    Average Score: 194.43</code></pre>
<h4 id="prioritized-sampling">Prioritized sampling</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">_agent_kwargs = &#123;</span><br><span class="line">    <span class="string">"state_size"</span>: env.observation_space.shape[<span class="number">0</span>],</span><br><span class="line">    <span class="string">"action_size"</span>: env.action_space.n, </span><br><span class="line">    <span class="string">"number_hidden_units"</span>: <span class="number">64</span>,</span><br><span class="line">    <span class="string">"optimizer_fn"</span>: optimizer_fn,</span><br><span class="line">    <span class="string">"epsilon_decay_schedule"</span>: epsilon_decay_schedule,</span><br><span class="line">    <span class="string">"alpha"</span>: <span class="number">0.5</span>,</span><br><span class="line">    <span class="string">"batch_size"</span>: <span class="number">64</span>,</span><br><span class="line">    <span class="string">"buffer_size"</span>: <span class="number">100000</span>,</span><br><span class="line">    <span class="string">"beta_annealing_schedule"</span>: <span class="keyword">lambda</span> n: exponential_annealing_schedule(n, <span class="number">1e-2</span>),</span><br><span class="line">    <span class="string">"gamma"</span>: <span class="number">0.99</span>,</span><br><span class="line">    <span class="string">"update_frequency"</span>: <span class="number">4</span>,</span><br><span class="line">    <span class="string">"seed"</span>: <span class="literal">None</span>,</span><br><span class="line">&#125;</span><br><span class="line">double_dqn_agent = DeepQAgent(**_agent_kwargs)</span><br><span class="line"></span><br><span class="line">prioritized_sampling_scores = train(double_dqn_agent,</span><br><span class="line">                                    env,</span><br><span class="line">                                    <span class="string">"prioritized-sampling-checkpoint.pth"</span>,</span><br><span class="line">                                    number_episodes=<span class="number">2000</span>,</span><br><span class="line">                                    target_score=float(<span class="string">"inf"</span>))</span><br></pre></td></tr></table></figure>
<pre><code>Episode 100 Average Score: -130.92
Episode 200 Average Score: 15.15
Episode 300 Average Score: 71.90
Episode 400 Average Score: 191.28
Episode 500 Average Score: 226.89
Episode 600 Average Score: 238.60
Episode 700 Average Score: 194.70
Episode 800 Average Score: 227.00
Episode 900 Average Score: 223.78
Episode 1000    Average Score: 228.35
Episode 1100    Average Score: 220.97
Episode 1200    Average Score: 220.92
Episode 1300    Average Score: 241.30
Episode 1400    Average Score: 209.41
Episode 1500    Average Score: 200.94
Episode 1600    Average Score: 202.66
Episode 1700    Average Score: 235.63
Episode 1800    Average Score: 249.41
Episode 1900    Average Score: 237.83
Episode 2000    Average Score: 259.26</code></pre>
<h4 id="plotting-the-time-series-of-scores">Plotting the time series of scores</h4>
<p>I can use <a href="https://pandas.pydata.org/" target="_blank" rel="noopener">Pandas</a> to quickly plot the time series of scores along with a 100 episode moving average. Most obvious difference between the two different sampling strategies is that prioritized sampling reduces, perhaps even eliminates, the significant number of large negative scores. Perhaps this is because prioritized sampling replays exactly those experiences that generate, at least initially, large losses (in magnitude). Over time the RL agent using prioritized sampling learns how to handle those awkward state transitions that led to <em>really</em> poor scores much better than an RL agent that uses uniform random sampling throughout.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">uniform_sampling_scores = pd.Series(uniform_sampling_scores, name=<span class="string">"scores"</span>)</span><br><span class="line">prioritized_sampling_scores = pd.Series(prioritized_sampling_scores, name=<span class="string">"scores"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">fig, axes = plt.subplots(<span class="number">2</span>, <span class="number">1</span>, figsize=(<span class="number">10</span>, <span class="number">6</span>), sharex=<span class="literal">True</span>, sharey=<span class="literal">True</span>)</span><br><span class="line">_ = uniform_sampling_scores.plot(ax=axes[<span class="number">0</span>], label=<span class="string">"Uniform Sampling"</span>)</span><br><span class="line">_ = (uniform_sampling_scores.rolling(window=<span class="number">100</span>)</span><br><span class="line">               .mean()</span><br><span class="line">               .rename(<span class="string">"Rolling Average"</span>)</span><br><span class="line">               .plot(ax=axes[<span class="number">0</span>]))</span><br><span class="line">_ = axes[<span class="number">0</span>].legend()</span><br><span class="line">_ = axes[<span class="number">0</span>].set_ylabel(<span class="string">"Score"</span>)</span><br><span class="line"></span><br><span class="line">_ = prioritized_sampling_scores.plot(ax=axes[<span class="number">1</span>], label=<span class="string">"Double DQN Scores"</span>)</span><br><span class="line">_ = (prioritized_sampling_scores.rolling(window=<span class="number">100</span>)</span><br><span class="line">                      .mean()</span><br><span class="line">                      .rename(<span class="string">"Rolling Average"</span>)</span><br><span class="line">                      .plot(ax=axes[<span class="number">1</span>]))</span><br><span class="line">_ = axes[<span class="number">1</span>].legend()</span><br><span class="line">_ = axes[<span class="number">1</span>].set_ylabel(<span class="string">"Score"</span>)</span><br><span class="line">_ = axes[<span class="number">1</span>].set_xlabel(<span class="string">"Episode Number"</span>)</span><br></pre></td></tr></table></figure>
<p>​<br />
<img src="https://i.imgur.com/W9KbFXx.png" alt="png" /> ​</p>
<h4 id="kernel-density-plot-of-the-scores">Kernel density plot of the scores</h4>
<p>In general, the kernel density plot will be bimodal with one mode less than -100 and a second mode greater than 200. The negative mode corresponds to those training episodes where the agent crash landed and thus scored at most -100; the positive mode corresponds to those training episodes where the agent "solved" the task. The kernel density or scores typically exhibits negative skewness (i.e., a fat left tail): there are lots of ways in which landing the lander can go horribly wrong (resulting in the agent getting a very low score) and only relatively few paths to a gentle landing (and a high score).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">_ = uniform_sampling_scores.plot(kind=<span class="string">"kde"</span>, ax=ax, label=<span class="string">"Uniform Sampling"</span>)</span><br><span class="line">_ = prioritized_sampling_scores.plot(kind=<span class="string">"kde"</span>, ax=ax, label=<span class="string">"Priority Sampling"</span>)</span><br><span class="line">_ = ax.set_xlabel(<span class="string">"Score"</span>)</span><br><span class="line">_ = ax.set_xscale(<span class="string">"symlog"</span>)</span><br><span class="line">_ = ax.legend()</span><br></pre></td></tr></table></figure>
<p>​<br />
<img src="https://i.imgur.com/ObFtMi7.png" alt="png" /> ​</p>
<h2 id="where-to-go-from-here">Where to go from here?</h2>
<p>Up next in this series will be <a href="https://arxiv.org/abs/1511.06581" target="_blank" rel="noopener"><em>Dueling Network Architectures for Deep Reinforcement Learning</em></a>.</p>

    </div>

    
    
    
        <div class="reward-container">
  <div>谢谢老板🐱~.</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    Donate
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/reward.png" alt="Nansong Yi WeChat Pay">
        <p>WeChat Pay</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.png" alt="Nansong Yi Alipay">
        <p>Alipay</p>
      </div>

  </div>
</div>

        

  <div class="followme">
    <p>Welcome to my other publishing channels</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/rss.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/08/07/HarDNet/" rel="prev" title="HarDNet">
      <i class="fa fa-chevron-left"></i> HarDNet
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/10/13/coding-interview/" rel="next" title="coding interview">
      coding interview <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#经典dqn"><span class="nav-number">1.</span> <span class="nav-text">经典DQN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ddqn-with-prioritized-experience-replay"><span class="nav-number">2.</span> <span class="nav-text">DDQN with Prioritized Experience Replay</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#improving-the-double-dqn-algorithm-using-prioritized-experience-replay"><span class="nav-number"></span> <span class="nav-text">Improving the Double DQN algorithm using prioritized experience replay</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#prioritized-experience-replay"><span class="nav-number">1.</span> <span class="nav-text">Prioritized Experience Replay</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#prioritization-using-the-temporal-difference-td-error-term"><span class="nav-number">1.1.</span> <span class="nav-text">Prioritization using the temporal-difference (TD) error term</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#stochastic-prioritization"><span class="nav-number">1.2.</span> <span class="nav-text">Stochastic prioritization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#correcting-for-sampling-bias"><span class="nav-number">1.3.</span> <span class="nav-text">Correcting for sampling bias</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#implementation"><span class="nav-number">1.4.</span> <span class="nav-text">Implementation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#refactoring-the-deepqagent-class"><span class="nav-number">2.</span> <span class="nav-text">Refactoring the DeepQAgent class</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-training-loop"><span class="nav-number">3.</span> <span class="nav-text">The Training Loop</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#solving-the-lunarlander-v2-environment"><span class="nav-number">4.</span> <span class="nav-text">Solving the LunarLander-v2 environment</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#action-space"><span class="nav-number">4.1.</span> <span class="nav-text">Action Space</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#colab-specific-environment-setup"><span class="nav-number">4.2.</span> <span class="nav-text">Colab specific environment setup</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#creating-a-deepqagent"><span class="nav-number">4.3.</span> <span class="nav-text">Creating a DeepQAgent</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#beta-annealing-schedule"><span class="nav-number">4.3.1.</span> <span class="nav-text">\(\beta\)-annealing schedule</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#epsilon-decay-schedule"><span class="nav-number">4.3.2.</span> <span class="nav-text">\(\epsilon\)-decay schedule</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#choosing-an-optimizer"><span class="nav-number">4.3.3.</span> <span class="nav-text">Choosing an optimizer</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#training-the-deepqagent"><span class="nav-number">4.4.</span> <span class="nav-text">Training the DeepQAgent</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#uniform-random-sampling"><span class="nav-number">4.4.1.</span> <span class="nav-text">Uniform random sampling</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#prioritized-sampling"><span class="nav-number">4.4.2.</span> <span class="nav-text">Prioritized sampling</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#plotting-the-time-series-of-scores"><span class="nav-number">4.4.3.</span> <span class="nav-text">Plotting the time series of scores</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#kernel-density-plot-of-the-scores"><span class="nav-number">4.4.4.</span> <span class="nav-text">Kernel density plot of the scores</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#where-to-go-from-here"><span class="nav-number">5.</span> <span class="nav-text">Where to go from here?</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Nansong Yi"
      src="/images/baby.png">
  <p class="site-author-name" itemprop="name">Nansong Yi</p>
  <div class="site-description" itemprop="description">TheSong is named after IG.TheShy</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">14</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/nansongyi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;nansongyi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:nansong@uw.edu" title="E-Mail → mailto:nansong@uw.edu" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://nansongyi.me/" title="http:&#x2F;&#x2F;nansongyi.me" rel="noopener" target="_blank">my web</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://silencial.github.io/" title="https:&#x2F;&#x2F;silencial.github.io&#x2F;" rel="noopener" target="_blank">silencial</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://3monks.org/en/" title="https:&#x2F;&#x2F;3monks.org&#x2F;en&#x2F;" rel="noopener" target="_blank">sky</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.twistedwg.com/pages/class.html" title="http:&#x2F;&#x2F;www.twistedwg.com&#x2F;pages&#x2F;class.html" rel="noopener" target="_blank">twistedwg</a>
        </li>
    </ul>
  </div>

      </div>
      <!--
      <div style="position:absolute; bottom:120px left:auto; width:85%">
          <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=450 src="//music.163.com/outchain/player?type=0&id=5290225297&auto=1&height=430"></iframe>
      </div>
      -->

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Nansong Yi</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

    </div>
</body>
</html>
